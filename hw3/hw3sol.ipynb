{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat 257 Homework 3\n",
    "\n",
    "**Due May 6 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a linear mixed effects model\n",
    "$$\n",
    "    \\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma} + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where   \n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,  \n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effect predictor matrix of $i$-th individual,  \n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effect predictor matrix of $i$-th individual,  \n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$,  \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and  \n",
    "- $\\boldsymbol{\\gamma} \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Revise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Revise may not be needed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 Formula (10 pts)\n",
    "\n",
    "Write down the log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ given parameters $(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2)$. \n",
    "\n",
    "**Hint:** For non-statisticians, feel free to ask for help in class or office hours. Point of this exercise is computing not statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "Notice that the sum of two independent normal random variables is also a normal random variable. Hence, $Z_{i}\\gamma \\sim N(0_{n_{i}}, Z_{i}\\Sigma Z_{i}^{T})$, $Z_{i}\\gamma + \\epsilon_{i} \\sim N(0_{n_{i}}, Z_{i}\\Sigma Z_{i}^{T} + \\sigma^{2}I_{n_{i}})$, and $Y_{i} \\sim N(X_{i}\\beta, Z_{i}\\Sigma Z_{i}^{T} + \\sigma^{2}I_{n_{i}})$.\n",
    "\n",
    "The likelihood of the $i$th datum proceeds:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "f(y_{i} \\vert \\beta,\\Sigma, \\sigma^{2}) &=& -\\frac{1}{(2\\pi)^{n_{i}/2}\\lvert Z_{i}\\Sigma Z_{i}^{T} + \\sigma^{2}I_{n_{i}}\\rvert^{1/2}}\\exp\\left(-\\frac{1}{2}(y_{i} - X_{i}\\beta)^{T}(Z_{i}\\Sigma Z_{i}^{T} + \\sigma^{2}I_{n_{i}})^{-1}(y_{i} - X_{i}\\beta)\\right)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The corresponding log-likelihood would be acquired by taking the log of the likelihood:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "l(\\beta, \\Sigma, \\sigma^{2}\\vert y_{i}) &=& -\\frac{n_{i}}{2}\\log(2\\pi) - \\frac{1}{2}\\log \\lvert Z_{i}\\Sigma Z_{i}^{T} + \\sigma^{2}I_{n_{i}}\\rvert - \\frac{1}{2}(y_{i} - X_{i}\\beta)^{T}(Z_{i}\\Sigma Z_{i}^{T} + \\sigma^{2}I_{n_{i}})^{-1}(y_{i} - X_{i}\\beta)\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the computation of the log-likelihood in the following parts, we proceed by simplifying $(Z_{i}\\Sigma Z_{i}^{T} + \\sigma^{2}I_{n_{i}})^{-1}$ via Sherman-Morrison-Woodbury. We also factor $\\Sigma = LL^{T}$ to avoid inverting $\\Sigma$:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "(Z_{i}\\Sigma Z_{i}^{T} + \\sigma^{2}I_{n_{i}})^{-1} &=& (Z_{i}LL^{T}Z_{i}^{T} + \\sigma^{2}I_{n_{i}})^{-1}\\\\\n",
    " &=& \\sigma^{-2}I_{n_{i}} - \\sigma^{-2}I_{n_{i}}Z_{i}L(I_{q} + \\sigma^{-2}LZ_{i}^{T}Z_{i}L)^{-1}L^{T}Z_{i}^{T}\\sigma^{-2}I_{n_{i}}\\\\\n",
    " &=& \\sigma^{-2}\\left(I_{n_{i}} - Z_{i}L(\\sigma^{2}I_{q} + L^{T}Z_{i}^{T}Z_{i}L)^{-1}L^{T}Z_{i}^{T}\\right)\\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further facilitate the computation of the log-likelihood, we turn to Hw1 part 4. We use the Weinstein-Aronzayjn identity to simplify further:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\lvert Z_{i}\\Sigma Z_{i}^{T} + \\sigma^{2}I_{n_{i}}\\rvert &=& \\sigma^{2n_{i}}\\lvert I_{n_{i}} + \\sigma^{-2}Z_{i}\\Sigma Z_{i}^{T}\\rvert\\\\\n",
    " &=& \\sigma^{2n_{i}}\\lvert I_{q} + \\sigma^{-2}\\Sigma Z_{i}^{T}Z_{i}\\rvert\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the final log likelihood is:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "l(\\beta, \\Sigma, \\sigma^{2}\\vert y_{i}) &=& -\\frac{n_{i}}{2}\\log(2\\pi) - \\frac{1}{2}\\log \\left(\\sigma^{2n_{i}}\\lvert I_{q} + \\sigma^{-2}\\Sigma Z_{i}^{T}Z_{i}\\rvert\\right) - \\frac{1}{2}(y_{i} - X_{i}\\beta)^{T}\\sigma^{-2}\\left(I_{n_{i}} - Z_{i}L(\\sigma^{2}I_{q} + L^{T}Z_{i}^{T}Z_{i}L)^{-1}L^{T}Z_{i}^{T}\\right)(y_{i} - X_{i}\\beta)\\\\\n",
    " &=& -\\frac{n_{i}}{2}\\log(2\\pi) - \\frac{n_{i}}{2}\\log \\sigma^{2} - \\frac{1}{2}\\log \\lvert I_{q} + \\sigma^{-2}\\Sigma Z_{i}^{T}Z_{i}\\rvert - \\frac{1}{2\\sigma^{2}}\\left((y_{i} - X_{i}\\beta)^{T}(y_{i} - X_{i}\\beta) - (y_{i} - X_{i}\\beta)^{T}Z_{i}L(\\sigma^{2}I_{q} + L^{T}Z_{i}^{T}Z_{i}L)^{-1})^{-1}L^{T}Z_{i}^{T}(y_{i} - X_{i}\\beta)\\right)\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Start-up code\n",
    "\n",
    "Use the following template to define a type `LmmObs` that holds an LMM datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmObs"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y :: Vector{T}\n",
    "    X :: Matrix{T}\n",
    "    Z :: Matrix{T}\n",
    "    # working arrays\n",
    "    # whatever intermediate vectors/arrays you may want to pre-allocate\n",
    "    storage_p  :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    yty        :: T\n",
    "    xty        :: Vector{T}\n",
    "    zty        :: Vector{T}\n",
    "    xtx        :: Matrix{T}\n",
    "    ztx        :: Matrix{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "end\n",
    "\n",
    "# constructor\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}\n",
    "        ) where T <: AbstractFloat\n",
    "    storage_p  = Vector{T}(undef, size(X, 2))\n",
    "    storage_q  = Vector{T}(undef, size(Z, 2))\n",
    "    yty        = dot(y, y)\n",
    "    xty        = transpose(X) * y\n",
    "    zty        = transpose(Z) * y\n",
    "    xtx        = transpose(X) * X\n",
    "    ztx        = transpose(Z) * X\n",
    "    ztz        = transpose(Z) * Z\n",
    "    storage_qq = similar(ztz)\n",
    "    LmmObs(y, X, Z, storage_p, storage_q, yty, xty, zty, \n",
    "        xtx, ztx, ztz, storage_qq)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function, with interface   \n",
    "```julia\n",
    "logl!(obs, β, L, σ²)\n",
    "```\n",
    "that evaluates the log-likelihood of the $i$-th datum. Here `L` is the lower triangular Cholesky factor from the Cholesky decomposition `Σ=LL'`. Make your code efficient in the $n_i \\gg q$ case. Think the intensive longitudinal measurement setting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl! (generic function with 1 method)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "import LinearAlgebra: BlasReal, copytri!\n",
    "\n",
    "function logl!(\n",
    "        obs :: LmmObs{T}, \n",
    "        β   :: Vector{T}, \n",
    "        L   :: Matrix{T}, \n",
    "        σ²  :: T) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2)    \n",
    "    # compute and return the log-likelihood\n",
    "    # starting constants\n",
    "    l = -(n * (log(2) + log(pi)) + (n - q) * log(σ²)) / 2\n",
    "#    l -= n / 2 * log(2 * pi) + n / 2 * log(σ²)\n",
    "    \n",
    "    # compute V = σ²I_q + L'Zᵢ'ZᵢL to compute determinant.\n",
    "    copy!(obs.storage_qq, obs.ztz)\n",
    "    BLAS.trmm!('L', 'L', 'T', 'N', one(T), L, obs.storage_qq)\n",
    "    BLAS.trmm!('R', 'L', 'N', 'N', one(T), L, obs.storage_qq)\n",
    "#    obs.storage_qq .+= s2I_q\n",
    "    @inbounds for j in 1:q\n",
    "        obs.storage_qq[j, j] += σ²\n",
    "    end\n",
    "    #note: numerical precision issues were fixed after the same matrix was used\n",
    "    # to compute the determinant.\n",
    "    # Use diagonal elements of cholesky factorization\n",
    "    #note: computing logdet allocates. Don't need LU on this. \n",
    "    LAPACK.potrf!('L', obs.storage_qq)\n",
    "#    qr!(obs.storage_qq)\n",
    "    @inbounds for j in 1:q\n",
    "        l -= log(obs.storage_qq[j, j])\n",
    "    end\n",
    "\n",
    "    # note: Don't invert the matrix. Just do triangular (or symmetric?) solve.\n",
    "    \n",
    "    #add quadratic terms\n",
    "    copy!(obs.storage_q, obs.zty)\n",
    "    mul!(obs.storage_q, obs.ztx, β, -one(T), one(T))\n",
    "#    obs.storage_q .-= obs.ztx * β\n",
    "    # compute quadratic terms for (yᵢ - Xᵢβ)'(yᵢ - Xᵢβ)\n",
    "#    quadterm = 0.\n",
    "    # use the unused obs.storage_p to store the quadratic term\n",
    "#    mul!(obs.storage_p, obs.xtx, β)\n",
    "#    obs.storage_p .-= obs.xty\n",
    "#    obs.storage_p .-= obs.xty\n",
    "#    quad = obs.yty + dot(β, obs.storage_p)# - 2 * dot(obs.xty, β)\n",
    "    quad = obs.yty + dot(β, obs.xtx, β) - 2 * dot(obs.xty, β)\n",
    "    \n",
    "    # get L'Z'(yᵢ - Xᵢβ)\n",
    "    BLAS.trmv!('L', 'T', 'N', L, obs.storage_q) # TODO: gemv! instead?\n",
    "    # Calling V = σ²I + L'Zᵢ'ZᵢL, let L_V be the lower triangular matrix\n",
    "    # from the Cholesky of V\n",
    "    # and get (L_V)⁻¹L'Z'(yᵢ - Xᵢβ)\n",
    "    BLAS.trsv!('L', 'N', 'N', obs.storage_qq, obs.storage_q)\n",
    "\n",
    "    # aggregate quadratic terms for (yᵢ - Xᵢβ)'ZL(L_V)⁻ᵀ(L_V)⁻¹L'Z'(yᵢ - Xᵢβ)\n",
    "    quad -= dot(obs.storage_q, obs.storage_q)\n",
    "    \n",
    "#    l += dot(obs.zty, obs.storage_qq, obs.zty) / (2 * σ²)\n",
    "#    # store Zᵢ'Xᵢβ in obs.storage_q\n",
    "#    mul!(obs.storage_q, obs.ztx, β)\n",
    "#    l -= dot(obs.zty, obs.storage_qq, obs.storage_q) / σ²\n",
    "#    l += dot(obs.storage_q, obs.storage_qq, obs.storage_q) / (2 * σ²)\n",
    "    l -= quad / (2 * σ²)\n",
    "    \n",
    "#    sleep(1e-3) # wait 1 ms as if your code takes 1ms\n",
    "    return l\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: This function shouldn't be very long. Mine, obeying 80-character rule, is 25 lines. If you find yourself writing very long code, you're on the wrong track. Think about algorithm first then use BLAS functions to reduce memory allocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 Correctness (15 pts)\n",
    "\n",
    "Compare your result (both accuracy and timing) to the [Distributions.jl](https://juliastats.org/Distributions.jl/stable/multivariate/#Distributions.AbstractMvNormal) package using following data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmObs{Float64}([-1.450910909560209, 1.5185224894450862, 5.265021705624027, 4.485272594164557, 0.6949699666429332, 1.7723256696372407, 1.1065838446466518, 3.7291668118296073, 4.288899999400642, 2.8241842645202406  …  4.058027151891635, 1.0909724390970443, 0.026692243086209766, -0.8927757653299448, 6.94725248926293, 3.519302085567343, 4.914007299083773, 2.1610206566690797, 1.857389542694909, 6.513818951020866], [1.0 0.6790633442371218 … 0.5400611947971554 -0.632040682052606; 1.0 1.2456776800889142 … -0.4818455756130373 0.6467830314674976; … ; 1.0 0.0733124748775436 … 0.6125080259511859 0.4181258283983667; 1.0 -1.336609049786048 … -0.18567490803712938 1.0745977099307227], [1.0 -1.0193326822839996 -0.15855601254314888; 1.0 1.7462667837699666 -0.4584376230657152; … ; 1.0 1.4843185594903878 0.42458303115266854; 1.0 0.3791714762820068 0.25150666970865837], [2.252835259e-314, 2.2504600604e-314, 2.2525144965e-314, 2.250074539e-314, 2.2507000893e-314], [0.0, 0.0, 2.121995791e-314], 20378.320552515157, [4209.066417097508, -1849.2613478849603, 1339.1230193223864, 396.4337131159359, 1659.0630888170554], [4209.066417097506, -447.62664690981126, -2763.9856125358633], [2000.0 -16.870943820386746 … -4.678756487678486 -33.01394193208299; -16.870943820386746 1972.1480562703996 … 42.18373658967013 -18.842752802403005; … ; -4.678756487678486 42.18373658967013 … 1962.031706211974 31.9476748423444; -33.01394193208299 -18.842752802403005 … 31.9476748423444 1952.6108146037136], [2000.0 -16.870943820386746 … -4.678756487678486 -33.01394193208297; 3.207786785008377 51.535093570134116 … -5.599636592147196 -25.64263779027383; 5.8864999631191735 -108.82214158453627 … 84.16510226928119 -24.939700580608633], [2000.0 3.207786785008377 5.8864999631191735; 3.207786785008377 1988.894096853326 -57.3189115446946; 5.8864999631191735 -57.3189115446946 2009.451939652711], [5.0e-324 2.0e-323 3.5e-323; 1.0e-323 2.5e-323 4.0e-323; 1.5e-323 3.0e-323 8.289173e-317])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools, Distributions, LinearAlgebra, Random\n",
    "\n",
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)]\n",
    "Z  = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form an LmmObs object\n",
    "obs = LmmObs(y, X, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the standard way to evaluate log-density of a multivariate normal, using the Distributions.jl package. Let's evaluate the log-likelihood of this datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3256.179335805832"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "μ  = X * β\n",
    "Ω  = Z * Σ * transpose(Z) +  σ² * I\n",
    "mvn = MvNormal(μ, Symmetric(Ω)) # MVN(μ, Σ)\n",
    "logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your answer matches that from Distributions.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3256.1793358058253"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = Matrix(cholesky(Σ).L)\n",
    "logl!(obs, β, L, σ²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You will lose all 15 + 30 + 30 = 75 points** if the following statement throws `AssertionError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert logl!(obs, β, Matrix(cholesky(Σ).L), σ²) ≈ logpdf(mvn, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Efficiency (30 pts)\n",
    "\n",
    "Benchmarking your code and compare to the Distributions.jl function `logpdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 7015 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m599.002 μs\u001b[22m\u001b[39m … \u001b[35m  9.556 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 90.84%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m668.593 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m701.016 μs\u001b[22m\u001b[39m ± \u001b[32m142.152 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.18% ±  1.08%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\u001b[39m▆\u001b[39m█\u001b[39m▆\u001b[39m▄\u001b[39m▂\u001b[34m▁\u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[32m▅\u001b[39m\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m▂\n",
       "  599 μs\u001b[90m           Histogram: frequency by time\u001b[39m         1.09 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m31.53 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m4\u001b[39m."
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark the `logpdf` function in Distribution.jl\n",
    "bm1 = @benchmark logpdf($mvn, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 149 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m672.718 ns\u001b[22m\u001b[39m … \u001b[35m  3.221 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m712.661 ns               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m791.899 ns\u001b[22m\u001b[39m ± \u001b[32m184.771 ns\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m▆\u001b[39m█\u001b[34m▆\u001b[39m\u001b[39m▅\u001b[39m▃\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[32m▂\u001b[39m\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\n",
       "  \u001b[39m▅\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m \u001b[39m█\n",
       "  673 ns\u001b[90m        \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m       1.52 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m0 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m0\u001b[39m."
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark your implementation\n",
    "L = Matrix(cholesky(Σ).L)\n",
    "bm2 = @benchmark logl!($obs, $β, $L, $σ²)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points you will get is\n",
    "$$\n",
    "\\frac{x}{1000} \\times 30,\n",
    "$$\n",
    "where $x$ is the speedup of your program against the standard method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.144921529572972"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you'll get\n",
    "clamp(median(bm1).time / median(bm2).time / 1000 * 30, 0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: Apparently I am using 1000 as denominator because I expect your code to be at least $1000 \\times$ faster than the standard method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 Memory (30 pts)\n",
    "\n",
    "You want to avoid memory allocation in the \"hot\" function `logl!`. You will lose 1 point for each `1 KiB = 1024 bytes` memory allocation. In other words, the points you get for this question is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(30 - median(bm2).memory / 1024, 0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: I am able to reduce the memory allocation to 0 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 Misc (15 pts)\n",
    "\n",
    "Coding style, Git workflow, etc. For reproducibity, make sure we (TA and myself) can run your Jupyter Notebook. That is how we grade Q4 and Q5. If we cannot run it, you will get zero points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
